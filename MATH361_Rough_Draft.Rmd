---
title: "Predicting Bike Share Ridership based on Weather Data in Seattle"
author: "Joey Rodriguez and Daniel Bhatti"
date: "2024-11-22"
output:
  pdf_document: default
  html_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#imports 
library(readr)
library(dplyr)
library(car)
library(tinytex)
library(ggplot2)
library(patchwork)
library(zoo)
library(knitr)
library(MASS)
library(olsrr)
library(leaps)
library(lubridate)
```

# Introduction
Bike share has launched in many U.S. cities since its introduction in Washington, D.C. in 2010 (1). One iteration of bike share  was Pronto! in downtown Seattle, Washington. From 2014 to 2017, 500 Pronto! bikes operated across 54 stations on the ithsmus. The City of Seattle, in partnership with Socratica, collected system data during the operating window and made it publicly available via its open data platform. Pronto! fell short of the success realized by other bike schemes in the U.S. like Capital Bikeshare, Philly’s Indego, and NYC’s CitiBike. Researchers have used system data to conduct a post-mortem analysis on Pronto! as dockless bike share schemes like Lime Scooters filled the void left by Pronto (2). In this brief paper, we  investigate the relationship between weather in the service area and daily ridership. In particular, we predict daily ridership based on weather data and time of year.


```{r, include=FALSE, fig.cap = "PRETTY FIGURE", out.width = "50%"}
knitr::include_graphics("pronto.png")
```

# Exploratory Data Analysis
## Data Cleaning
The data \texttt{trip.csv} and \texttt{weather.csv.xls} were downloaded from Kaggle (3). The \texttt{trip} data frame contains 275,091 cases (or rides) and 12 variables describing each ride. These data were collected over 901 days from 13 October 2014 to 31 March 2017. The relevant variables from the original 12 in this dataset are \texttt{start\_time} (day and time trip started, in PST) and \texttt{trip\_duration} (time of trip in seconds). The \texttt{weather} data frame contains 689 cases (or days) and 21 variables describing the weather that day. These data were collected from 13 October 2014 to 31 August 2016, or 689 days. Notice that the dates covered by the \texttt{weather} data set are a proper subset of the dates covered by the \texttt{trip} data set. 

We began by aggregating trip data for each day we have data for. From the \texttt{trip} data frame, we created a new data frame called \texttt{ridership} that aggregates trips by day. At the end of this, \texttt{ridership} has 901 rows (days) and 3 columns (variables): \texttt{count}, \texttt{tripduration}, and \texttt{day\_number}. Because the \texttt{trip} data covers 212 days after the last observation in the \texttt{weather} data, we want to keep only the observations in \texttt{trip} that match the observations in the smaller data frame, \texttt{weather}. We created our final data frame, \texttt{df}, by left-joining weather and ridership by \texttt{day\_number}. The final data frame contains 689 rows (days) and 28 columns (variables). The variable names are listed in the table below with brief descriptions.

```{r mapping, include=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1

trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") # strips the date from its current format
unique_dates <- sort(unique(trip$date)) # this collects unique dates
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) # this maps unique date to the integers, starting at 1
trip$day_number = date_to_number[as.character(trip$date)] # this adds the integer mapping as a column, day_number
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)

# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = round(sum(tripduration),1),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data, include = FALSE}
weather = read_csv('weather.csv.xls')
# calculates temperature range for each day
weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF 
# strips the date from its current format
weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") 
# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]
weather = weather[,-1] # remove the old date
# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)
df$avg_durations = round(df$total_durations / df$total_trips, 1)
write.csv(df, "cleaned_data.csv", row.names = FALSE)
```

```{r add season variables, echo = FALSE}
df$weekday_weekend <- ifelse(weekdays(df$date) %in% c("Saturday", "Sunday"),1,0)
# Define a function to classify seasons based on actual start dates
get_season <- function(date) {
  year <- lubridate::year(date)
  spring_start <- as.Date(paste0(year, "-03-20"))
  summer_start <- as.Date(paste0(year, "-06-21"))
  fall_start   <- as.Date(paste0(year, "-09-22"))
  winter_start <- as.Date(paste0(year, "-12-21"))
  ifelse(date >= spring_start & date < summer_start, 0,  # Spring
  ifelse(date >= summer_start & date < fall_start,   1,  # Summer
  ifelse(date >= fall_start & date < winter_start,   2,  # Fall
  3)))  # Winter
}
# Apply the function to the 'date' variable
df$season <- sapply(df$date, get_season)
# Create fall/winter dummy: 1 if Fall or Winter, 0 otherwise
df$fall_winter <- ifelse(df$season %in% c(2, 3), 1, 0)
```

```{r rtable, echo = FALSE}
# Create a data frame with variable names and descriptions
variable_descriptions <- data.frame(
  Variable = c(
    "Max_Temperature_F", "Mean_Temperature_F", "Min_TemperatureF",
    "Max_Dew_Point_F", "MeanDew_Point_F", "Min_Dewpoint_F",
    "Max_Humidity", "Mean_Humidity", "Min_Humidity",
    "Max_Sea_Level_Pressure_In", "Mean_Sea_Level_Pressure_In", "Min_Sea_Level_Pressure_In",
    "Max_Visibility_Miles", "Mean_Visibility_Miles", "Min_Visibility_Miles",
    "Max_Wind_Speed_MPH", "Mean_Wind_Speed_MPH", "Max_Gust_Speed_MPH",
    "Precipitation_In", "Events", "temp_range", "date", "day_number",
    "total_trips", "total_durations", "average_durations", "weekday_weekend",
    "season", "fall_winter"
  ),
  Description = c(
    "Maximum temperature (°F)",
    "Mean temperature (°F)",
    "Minimum temperature (°F)",
    "Maximum dew point (°F)",
    "Mean dew point (°F)",
    "Minimum dew point (°F)",
    "Maximum humidity (%)",
    "Mean humidity (%)",
    "Minimum humidity (%)",
    "Maximum sea-level pressure (inches Hg)",
    "Mean sea-level pressure (inches Hg)",
    "Minimum sea-level pressure (inches Hg)",
    "Maximum visibility (miles)",
    "Mean visibility (miles)",
    "Minimum visibility (miles)",
    "Maximum wind speed (MPH)",
    "Mean wind speed (MPH)",
    "Maximum gust speed (MPH)",
    "Precipitation (inches)",
    "Weather events (e.g., Rain, Snow)",
    "Temperature range (°F)",
    "Date of the observation",
    "Days since 12 October 2014",
    "Count of total trips",
    "Sum of total duration for all trips (seconds)",
    "Average ride duration (seconds)",
    "Encodes weekends: 1 if Saturday or Sunday, 0 otherwise",
    "Encodes seasons: 0 if Spring, 1 if Summer, 2 if Fall, 3 if Winter", 
    "Encodes wet season: 1 if Fall or Winter, 0 if Summer or Spring"
  )
)
# Render the table using kable
knitr::kable(variable_descriptions, col.names = c("Variable", "Description"), caption = "Variable Descriptions (689 days, 28 variables)")
```

Notice that nine variables in our data dictionary were created from other variables:
\begin{itemize}
\item \texttt{temp\_range} was created by the difference: \texttt{Max\_Temperature\_F} - \texttt{Min\_TemperatureF} 
\item \texttt{date} strips \texttt{"\%m/\%d/\%Y"} from the full \texttt{starttime} \texttt{"\%m/\%d/\%Y \%H:\%M"}
\item \texttt{day\_number} are the days beginning 13 October 2014, the first day of observation
\item \texttt{total\_trips} are the total trips recorded for that day
\item \texttt{total\_durations} are the total durations for all trips that day, aggregating \texttt{tripduration}
\item \texttt{avg\_durations} are the average durations for a trip each day, dividing the sum of trip durations by the total number of trips each day
\item \texttt{weekday\_weekend}
\item \texttt{season} was created based on \texttt{date}, according the the summer and winter solstices and the spring and fall equinoxes in the Northern hemisphere.
\item \texttt{fall\_winter} was created based on whether the season was Fall or Winter, which roughly coincides with the wet season in the Puget Sound Region from October to April (SOURCE).
\end{itemize}


## Understanding Outliers

The figure below plots daily bike ridership in Seattle, with the total rides taken each day in blue circles and the sum of the durations of the rides taken each day in red triangles. This figure suggests that outliers in total riders tend to coincide with outliers in ride durations. For instance, the day with the highest bike riders -- 941 on Sunday, April 20, 2015 -- was also the day with the second highest sum of ride durations (359.7 hours). It's not clear from lookup what caused bike ridership to be so high on this day; like much of the data we gather from the real world, this result was influenced by many factors that day.

36 days earlier on Sunday, March 15, 2015 was the second-wettest March day on record in the Puget Sound Region (SOURCE). The rain was so severe that a mudslide occured in Western Seattle. Knowing this, you'd expect March 15 to have been a bad day for cycling. Only 34 trips took place on this day with a combined ride duration of just 6.3 hours. This was the second worst day for cycling behind Sunday, December 27, 2015 with just 30 trips and 4.5 hours. The coincidence between trips and durations explains the flattening of the data -- the decrease in variation from the mean -- for the average ride durations per day.


```{r sum stats,include = FALSE}
summary(df$total_trips)
summary(df$total_durations)
summary(df$avg_durations)
```


```{r plot_y, echo = FALSE, fig.cap = "Daily Bike Share Ridership and Durations in Seattle."}

par(mar = c(4, 4, 4, 4) + 0.1)  # Increase the right margin (4th value)

# Set up the plot
plot(df$day_number, df$total_trips, 
     xlab = "Days since 12 October 2014", 
     ylab = "Total Bike Ridership", 
     col = "black", 
     bg = "blue",
     pch = 21, 
     ylim = c(0, max(df$total_trips)))  # Primary Y-axis

# Overlay the second dataset
par(new = TRUE)  # Add a new plot without clearing the previous one
plot(df$day_number, df$total_durations, 
     xlab = "", 
     ylab = "", 
     col = "black", 
     bg = "red",
     pch = 24, 
     axes = FALSE,  # Suppress axes for this plot
     ylim = c(0, max(df$total_durations)))  # Secondary Y-axis scale

# Add the secondary Y-axis
axis(4)  # Right Y-axis in red
mtext("Total Durations (secs)", side = 4, line = 3)  # Label for right Y-axis

# Add a title for the entire plot
title(main = "Daily Bike Share Ridership in Seattle", line = 1, cex.main = 1.25)

# Add a legend to differentiate datasets
legend("topright", legend = c(expression(Sigma ~ "Riders"), expression(Sigma ~ "Durations (secs)")), 
       col = c("black", "black"), 
       pt.bg = c("blue", "red"),
       pch = c(21, 24), 
       bty = "n")  # No box around the legend

```



## Selecting the Response Variable
The three candidates for a good response variable were created from the ``trip.csv`` data set, described in the ``ridership`` data frame, and merged into our final data frame: ``total_trips``, ``trip_durations``, and ``avg_durations``. We briefly discuss the merits of each response variable before a quantitative judgement:
\begin{itemize}
\item \texttt{total\_trips} is the most intuitive measure for bike ridership on a given day. It directly answers the question ``How many trips were there?'' for a given day. It gives us a picture of how willing people in the service area were to hop on a bike.
\item \texttt{total\_durations} gives a more complete picture for the ridership on a given day. Once a rider hopped on a bike, how long did they ride before docking it? This gives us a picture of how willing riders in the service area were to stay on their bikes.
\item \texttt{avg\_durations} controls for the interaction between bike ridership and ridership durations. By dividing total ridership over total durations, we understand the willingness of those in the service area to both picking up a bike and keep riding on that bike.
\end{itemize}


```{r plot_normal_histograms, echo = FALSE, fig.cap = "Variables Measuring Bike Share Ridership in Seattle"}
par(mfrow = c(3, 2),       # Create a 3x2 grid
    mar = c(2, 2, 2, 1),   # Reduce margins around individual plots (bottom, left, top, right)
    oma = c(4, 4, 2, 2))   # Set outer margins for the entire figure

hist(df$total_trips, xlab='', main='Total Trips'); hist(log(df$total_trips), xlab='', main='log(Total Trips)')
hist(df$total_durations, xlab='', main='Total Durations'); hist(log(df$total_durations), xlab='', main='log(Total Durations)')
hist(df$avg_durations, xlab='', main='Average Durations'); hist(log(df$avg_durations), xlab='', main='log(Average Durations)')
```

We note (i) that \texttt{total\_trips} and \texttt{total\_durations} are highly correlated (>0.82), (ii) that average durations is bimodal, total durations is right-skewed, and total trips is roughly normal, and (iii) total durations makes for the easiest interpretation without being transformed. We therefore use \texttt{total\_trips} as our response variable going forward. 

The majority of our predictors are continuous variables. For each of the features with recorded min, mean, and max –- Visibility, Temperature, Dew Point, Humidity, and Sea Level Pressure -- we calculate their correlations with the response Total Trips. By feature, it turns out that Max Temperature, Min Visibility, Mean Dew Point, Mean Humidity, and Min Sea Level Pressure have the highest correlation with the response (Table 2). 

Total Trips' high correlation with Max Temperature may be explained by the temperature at midday, when it is usually highest. Midday may also be a peak time for ridership. We added each of these summary statistics with the highest correlation to Total Trips to our baseline model. Even though \texttt{Max\_Gust\_Speed\_MPH} had higher response correlation than either \texttt{Max\_Wind\_Speed\_MPH} or \texttt{Mean\_Wind\_Speed\_MPH}, it also had 410 missing values. We opted to add \texttt{Mean\_Wind\_Speed\_MPH} which had the next-highest response correlation (Figure 3). We also added Precipitation_In (zero-inflated continuous variable) and Events (dummy variable based on whether an event occurred that day).


```{r plot exploratory table, echo = FALSE}
# Create a new data frame with grouped features
compact_correlations <- data.frame(
  Feature = c("Temperature (°F)", "Visibility (miles)", "Dew Point (°F)", "Humidity (%)", "Sea Level Pressure (in)"),
  Min = c(
    round(cor(df$Min_TemperatureF, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Dewpoint_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Mean = c(
    round(cor(df$Mean_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$MeanDew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Max = c(
    round(cor(df$Max_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Dew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  )
)

# Create a table
knitr::kable(compact_correlations, caption = "Correlation between Weather Features and Total Trips")

```

```{r wind plots, echo = FALSE, warning=FALSE, fig.cap = "Effect of Wind Speed on Bike Share Ridership in Seattle."}
df$Max_Gust_Speed_MPH <- as.numeric(as.character(df$Max_Gust_Speed_MPH))
# Wind plots
par(mfrow=c(1,3)) # Set the layout for 3 plots in a single row
# Left-most plot
par(mar=c(5, 4, 4, 0.5)) # Bottom, Left, Top, Right margins
plot(df$Max_Wind_Speed_MPH, df$total_trips, 
     xlab="Max Wind Speed (MPH)", 
     ylab="Total Rides", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Wind_Speed_MPH)),")"))
# Middle plot 
par(mar=c(5, 2.25, 4, 2.25)) # Reduce left margin
plot(df$Mean_Wind_Speed_MPH, df$total_trips, 
     xlab="Mean Wind Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Mean_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Mean_Wind_Speed_MPH)),")"))
# Right-most plot
par(mar=c(5, 0.5, 4, 4)) # Reduce left margin
plot(df$Max_Gust_Speed_MPH, df$total_trips, 
     xlab="Max Gust Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Gust_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Gust_Speed_MPH)),")"))

```


# Model Selection

Our baseline model using the finding from exploratory data analysis is:

  

$$
\begin{aligned}
\texttt{Total\_Trips}_i &= \beta_{i0} + \beta_{i1} \texttt{Mean\_Humidity}_i + \beta_{i2} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i3} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i4} \texttt{Max\_Temperature\_F}_i + \beta_{i5} \texttt{Min\_Visibility\_Miles}_i \\
&+ \beta_{i6} \texttt{Min\_Sea\_Level\_Pressure\_In}_i + \beta_{i7} \texttt{Precipitation\_In}_i + \beta_{i8} \texttt{Events}_i \\
\end{aligned}
$$



We removed \texttt{Min\_Visibility\_Miles}, \texttt{Min\_Sea\_Level\_Pressure\_In} and \texttt{Events} because they were insignificant predictors for the response. Our full model at this point (RSE = 83.09, R2 = 0.7141, R2adj= 0.712, all terms significant to 0.01) is:

$$
\begin{aligned}
\texttt{Total\_Trips}_i^{full} &= \beta_{i0} + \beta_{i1} \texttt{Max\_Temperature\_F}_i + \beta_{i2} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i3} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i4} \texttt{Mean\_Humidity}_i + \beta_{i5} \texttt{Precipitation\_In}_i \\
\end{aligned}
$$
Transformations failed to produce superior models. Power Transform produced coefficients that were mostly close to one. Forward and backward selection both failed to produce a superior model, although they chose models similar to ours. 

A partial model (RSE = 83.45, R2 = 0.7112, R2adj= 0.7095, all terms infinitesimal
) without the term \texttt{Max\_Temperature\_F} had good summary statistics and diagnostic plots.

$$
\begin{aligned}
\texttt{Total\_Trips}_i^{part} &= \beta_{i0} + \beta_{i1} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i2} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i3} \texttt{Max\_Humidity}_i + \beta_{i4} \texttt{Precipitation\_In}_i \\
\end{aligned}
$$
Judged side by side, both models have good summary statistics, diagnostic plots, and interpretable coefficients. The full model provides a better fit over the partial model based on partial F-test comparison (F = 6.9579, p = 0.0085). However,
VIF is abnormally high for Mean_Humidity (VIF = 9.05), MeanDew_Point_F (VIF = 10.68), and Max_Temperature_F (VIF = 19.09) in the full model. It turns out that Max_Temperature_F has high correlation with MeanDew_Point_F (0.72) and Mean_Humidity (-0.67). VIF makes clear the confounding results in stepwise selection (Max_Temperature_F makes the best 1-predictor model). The partial model has VIF values near one. Thus, the partial model is the better model.

Finally, we strengthen our model by considering temporal variation. Adding \texttt{weekday\_weekend} accounts for weekly changes in ridership based on commute changes for leisure, work and school. Adding \texttt{season} accounts for changes in ridership based on the assumption of four seasons while adding \texttt{fall\_winter} accounts for changes in ridership based on the assumption of a wet and dry season:
$$
\begin{aligned}
\texttt{Total\_Trips}_i^{temp} &= \beta_{i0} + \beta_{i1} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i2} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i3} \texttt{Max\_Humidity}_i + \beta_{i4} \texttt{Precipitation\_In}_i \\
&+ \beta_{i5} \texttt{weekday\_weekend}_i + \beta_{i6} \texttt{season}_i + \beta_{i7} \texttt{fall\_winter}_i \\
\end{aligned}
$$
Considering both the high correlation between variables \texttt{season} and \texttt{fall\_winter} and the weakness of the \texttt{season} variable, we opt to drop this variable. Our final model is:

$$
\begin{aligned}
\texttt{Total\_Trips}_i^{fin} &= \beta_{i0} + \beta_{i1} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i2} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i3} \texttt{Max\_Humidity}_i + \beta_{i4} \texttt{Precipitation\_In}_i \\
&+ \beta_{i5} \texttt{weekday\_weekend}_i + \beta_{i6} \texttt{fall\_winter}_i \\
\end{aligned}
$$


```{r plot diag plots, echo=FALSE}
out = lm(total_trips ~ Mean_Temperature_F, data = df)
#par(mfrow=c(2,2))
#plot(out)
#summary(out)
#Adjusted R^2 is 0.5612

outHum = lm(total_trips ~ Mean_Humidity, data = df)
#par(mfrow=c(2,2))
#plot(outHum)
#summary(outHum)
#Diagnostic plots look incredible. Adj R^2 = 0.4611

outRain = lm(total_trips ~ Precipitation_In, data = df)
#par(mfrow=c(2,2))
#plot(outRain)
#summary(outRain)

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data =df)
#par(mfrow=c(2,2))
#plot(outWind)
#summary(outWind)
#Plots bad, R^2 <0.2
```

```{r modelling, include = FALSE}
rainResiduals <- resid(outRain)
windResiduals <- resid(outWind)

variance_rain = lm(abs(windResiduals) ~ Mean_Wind_Speed_MPH, data = df)

rainpredictedvar = predict(variance_rain)

rainweights = 1/(rainpredictedvar^2)

RainWLS <- lm(total_trips ~ Precipitation_In, data = df, weights = rainweights)

#RainWLS did not produce meaningful improvements.

outDew = lm(total_trips ~ MeanDew_Point_F, data = df)

#Great plots R^2 is only 0.2

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data = df)

#Good plots R^2 is a paltry 0.07

windweights <- 1/lm(abs(outWind$residuals)~outWind$fitted.values)$fitted.values^2

WindWLS <- lm(total_trips ~ Mean_Wind_Speed_MPH, data = df, weights = windweights)

#Using WLS on windspeed significantly improves it. The R^2 is 0.7

outRange = lm(total_trips ~ temp_range, data = df)

#Plots have 1 extremely influential point, the R^2 is 0.315
#If we are to use mean temp I think we can ignore this variable (as a basic regressor)

outVisible = lm(total_trips ~ Mean_Visibility_Miles, data = df)

#Plots are mid, R^2 is 0.1313

visresid =  resid(outVisible)

variance_vis = lm(abs(visresid)~Mean_Visibility_Miles, data=df)

vispredictvar = predict(variance_vis)

visweights = 1/(vispredictvar^2)

VisWLS <- lm(total_trips ~ Mean_Visibility_Miles, data = df, weights = visweights)

#Very unimpressive.

outSea = lm(total_trips ~ Mean_Sea_Level_Pressure_In, data = df)

#Criteria for being in full model is that it had a *** significance by itself

fullmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles, data = df)

#Very interestingly Mean temp is not significant. Additionally visibility is far from significant

#Diagnostic plots look very good. R^2 is 0.71

partialmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

partialmodel2 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#Good diagnostics, R^2 is 0.71, all regressors are significant.

#I think that this is the best model.

testmodel = lm(total_trips ~ Mean_Temperature_F + MeanDew_Point_F, data=df)

#R^2 0.63, good diagnostics.

testmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity, data=df)

#R^2 0.6487, good diagnostics

testmodel3 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F, data=df)

#Suddenly temperature is not significant


powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,(df$Precipitation_In+1),(df$Mean_Wind_Speed_MPH+1),df$Mean_Visibility_Miles))

#This fails as powerTransform needs arguments to be strictly positive and the min
#of Precipitation and Wind speed are 0

powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,df$Mean_Visibility_Miles))

#trips: 0.761, Temperature: 0.760, Humidity: 0.67, Dew point 1.1, Visibility 10

#Therefore trips 3/4, temperate 3/4, humidity 2/3, Dew point no change visibility, visibility^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^10

transform_out <- lm(total_trips_trans ~ Mean_Temperature_F_trans + Mean_Humidity_trans + MeanDew_Point_F + Mean_Visibility_Miles_trans, data = df)

#The transformed model is not very impressive. Good diagnostics, R^2 of 0.6484

partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)

#2nd best model

partialmodel4 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F+temp_range, data = df)

fullmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles+Mean_Sea_Level_Pressure_In, data = df)

vif(partialmodel3)

cor(df[, c("Mean_Humidity", "MeanDew_Point_F", "Max_Temperature_F", "temp_range")])

anova(partialmodel,fullmodel2)

df_clean <- na.omit(df)

fullmodel <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Events + Precipitation_In,
                data = df)

alias(fullmodel)

nullmodel <- lm(total_trips ~ 1, data = df)


#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodel))

forwardselectionmodel = lm(total_trips~Max_Temperature_F+Precipitation_In+Mean_Humidity
                           +Mean_Wind_Speed_MPH+Mean_Sea_Level_Pressure_In+Min_Visibility_Miles
                           +Max_Wind_Speed_MPH+Max_Humidity+Min_Dewpoint_F+Mean_Temperature_F, data=df)

#Bad

#stepAIC(fullmodel, direction = "backward")

backwardselectionmodel = lm(total_trips~ Max_Temperature_F+Mean_Temperature_F+Min_Dewpoint_F+Max_Humidity+Mean_Humidity+Mean_Sea_Level_Pressure_In
                            +Max_Wind_Speed_MPH+Mean_Wind_Speed_MPH+Precipitation_In, data=df)

#better but still worse than what we already have

all_subsets <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Events + Precipitation_In, 
                          data = df, nvmax = 10)  # nvmax specifies max number of predictors

# View a summary of the results
summary(all_subsets)

# Extract the summary
subsets_summary <- summary(all_subsets)

# Display key metrics
subsets_summary$adjr2  # Adjusted R^2 for each model
subsets_summary$bic    # BIC for each model
subsets_summary$cp     # Mallow's Cp for each model

best_adj_r2_index <- which.max(subsets_summary$adjr2)
cat("Best model by Adjusted R^2 is Model", best_adj_r2_index, "with Adjusted R^2:", max(subsets_summary$adjr2), "\n")

best_bic_index <- which.min(subsets_summary$bic)
cat("Best model by BIC is Model", best_bic_index, "with BIC:", min(subsets_summary$bic), "\n")

# View variables for the best model (by Adjusted R^2 as an example)
subsets_summary$outmat[best_adj_r2_index, ]

partialmodel5 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#This model does not have issues with multicolinearity

df$Event_Binary <- ifelse(!is.na(df$Events), 1, 0)

eventout = lm(total_trips~Event_Binary, data =df)

fullmodelevent <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Precipitation_In+Event_Binary,
                data = df)

#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodelevent))

#stepAIC(fullmodelevent, direction = "backward")

all_subsetsevent <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Event_Binary + Precipitation_In, 
                          data = df, nvmax = 10)

# Get the summary of the regsubsets output
subset_summary <- summary(all_subsetsevent)

# Extract metrics
rsquared <- subset_summary$rsq
adj_r_squared <- subset_summary$adjr2
bic <- subset_summary$bic
cp <- subset_summary$cp

# View the metrics
print(rsquared)
print(adj_r_squared)
print(bic)
print(cp)

# Best model by BIC (minimum value)
best_bic_model <- which.min(bic)

# Best model by adjusted R^2 (maximum value)
best_adjr2_model <- which.max(adj_r_squared)

# Best model by Cp (closest to the number of predictors)
best_cp_model <- which.min(abs(cp - subset_summary$np))

# Print the results
cat("Best model by BIC:", best_bic_model, "\n")
cat("Best model by Adjusted R^2:", best_adjr2_model, "\n")
cat("Best model by Cp:", best_cp_model, "\n")


# Extract coefficients for the best model (e.g., by BIC)
best_model_coeffs <- coef(all_subsetsevent, id = best_bic_model)

# View the coefficients
print(best_model_coeffs)

#Final model (best model)

modela = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

modelb = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+log(Precipitation_In+0.0001)+log(Mean_Wind_Speed_MPH+0.0001), data = df)

df$PrecipitationDummy <- ifelse(df$Precipitation_In > 0, 1, 0)

modelc = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+PrecipitationDummy+Mean_Wind_Speed_MPH, data = df)

df$EventsDummy <- ifelse(is.na(df$Events), 0, 1)

modeld = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Mean_Wind_Speed_MPH+EventsDummy, data = df)

df$RainDummy = ifelse(grepl("Rain", df$Events, fixed = TRUE), 1, 0)

modele= lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+RainDummy+Mean_Wind_Speed_MPH, data = df)

numeric_vars <- df[sapply(df, is.numeric)]

# Compute the correlation matrix
correlation_table <- cor(numeric_vars, use = "complete.obs")

# View the correlation table
print(correlation_table)



outggg = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+factor(season)+weekday_weekend, data = df)

#actual last model

veryfinalmodel = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+fall_winter+weekday_weekend, data = df)


```

```{r very final model, echo = FALSE}
partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)



veryfinalmodel = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+fall_winter+weekday_weekend, data = df)
#regression summary of the final model
#summary(partialmodel3)

# diagnostic plots
par(mfrow=c(2,2))
plot(veryfinalmodel)

# pairs plot
#pairs(~Mean_Humidity + MeanDew_Point_F + #Precipitation_In + Mean_Wind_Speed_MPH + #Max_Temperature_F, data = df)
```

The equation for our final regression model is:

  \texttt{total\_trips} = 277.1643 – 4.95(\texttt{Mean\_Humidity}) + 5.90(\texttt{MeanDew\_Point}) – 118.151(\texttt{Precipitation\_In}) – 7.91(\texttt{Mean\_Wind\_Speed}) + 2.94(\texttt{Max\_Temperature})

Each regressor is significant to at least the 0.01 level, and the diagnostic plots are satisfactory. The residual plot moving-average looks flat. The normal quantile plot has few departures from the line. The scale location plot is relatively flat, indicating constant variance across fitted values.


```{r, include=FALSE}
cor(df$total_durations,df$total_trips)
#Total trips and total durations are only 82.2% correlated

cor(df$total_durations, df$Mean_Temperature_F, use = "complete.obs")

cor(df$total_durations, df$MeanDew_Point_F)


```










## Cross Validation, AIC/BIC, PowerTransform, VIF, 

# Conclusion
## Interpretations
## Limitations
## Extensions


Our work here suggests a path forward for bike share systems looking to bolster their operations and planning with weather data. However, Seattle is a city with temperate weather/climate. These results are not readily generalizable to all cities because when its too hot, people will also not ride bike!
[INSERT PARAGRAPH SUMMARIZING CONCLUSIONS FROM THE RESEARCH]









# Appendix
## Code Used for Data Cleaning
```{r mapping 1 copy, eval=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1
# strips the date from its current format
trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") 
unique_dates <- sort(unique(trip$date)) # this collects unique dates
# this maps unique date to the integers, starting at 1
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) 
# this adds the integer mapping as a column, day_number
trip$day_number = date_to_number[as.character(trip$date)] 
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)
```

```{r mapping 2 copy, eval=FALSE}
# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = round(sum(tripduration), 1),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data copy, eval = FALSE}
weather = read_csv('weather.csv.xls')
# calculates temperature range for each day
weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF 
# strips the date from its current format
weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") 
# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]
weather = weather[,-1] # remove the old date
# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)
df$avg_durations = round(df$total_durations / df$total_trips, 1)
```

```{r add season variables copy, eval = FALSE}
df$weekday_weekend <- ifelse(weekdays(df$date) %in% c("Saturday", "Sunday"),1,0)
# Define a function to classify seasons based on actual start dates
get_season <- function(date) {
  year <- lubridate::year(date)
  spring_start <- as.Date(paste0(year, "-03-20"))
  summer_start <- as.Date(paste0(year, "-06-21"))
  fall_start   <- as.Date(paste0(year, "-09-22"))
  winter_start <- as.Date(paste0(year, "-12-21"))
  ifelse(date >= spring_start & date < summer_start, 0,  # Spring
  ifelse(date >= summer_start & date < fall_start,   1,  # Summer
  ifelse(date >= fall_start & date < winter_start,   2,  # Fall
  3)))  # Winter
}
# Apply the function to the 'date' variable
df$season <- sapply(df$date, get_season)
# Create fall/winter dummy: 1 if Fall or Winter, 0 otherwise
df$fall_winter <- ifelse(df$season %in% c(2, 3), 1, 0)
```
