---
title: "Predicting Bike Share Ridership based on Weather Data in Seattle"
author: "Joey Rodriguez and Daniel Bhatti"
date: "2024-11-22"
output:
  pdf_document: default
  html_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#imports 
library(readr)
library(dplyr)
library(car)
library(tinytex)
library(ggplot2)
library(patchwork)
library(zoo)
library(knitr)
library(MASS)
library(olsrr)
library(leaps)
```

# Introduction
Cycle share has launched in many U.S. cities since its introduction in Washington, D.C. in 2010 (1). One iteration of cycle share  was Pronto! in downtown Seattle, Washington. From 2014 to 2017, 500 Pronto! bikes operated across 54 stations on the ithsmus. The City of Seattle, in partnership with Socratica, collected system data during the operating window and made it public via its open data platform. Pronto! fell short of the success realized by other bike schemes in the U.S. like Capital Bikeshare, Philly’s Indego, and NYC’s CitiBike. Researchers have used system data to conduct a post-mortem analysis on Pronto! as dockless bike share schemes like Lime Scooters filled the void left by Pronto (2). In this paper, we will investigate the relationship between weather in the service area and daily ridership. In particular, we want to predict daily ridership based on the weather data.

The data were downloaded from Kaggle (3). The file \texttt{trip.csv} contains data on each trip from 13 October 2014 to 31 March 2017, or 901 days. Each case in this dataset is a trip, and there were 275,091 trips over the 901 days. The relevant variables from the original 12 in this dataset are the response variables \texttt{start\_time} (day and time trip started, in PST) and \texttt{trip\_duration} (time of trip in seconds). In the file \texttt{weather.csv.xls}, a single case corresponds to a single day. This file contains the weather data for each day from 13 October 2014 to 31 August 2016, or 689 days. That is, the dates covered by the \texttt{weather} data set are a proper subset of the dates covered by the \texttt{trip} data set. Each day has 21 variables describing its weather. After merging these data sets and creating some of our own variables, we move on to exploratory data analysis for the response variables and predictors. This exploration informed our choice for our final model.

[INSERT PARAGRAPH SUMMARIZING CONCLUSIONS FROM THE RESEARCH]


```{r, include=FALSE, fig.cap = "PRETTY FIGURE", out.width = "50%"}
knitr::include_graphics("pronto.png")
```

# Exploratory Data Analysis

## Data Cleaning
```{r mapping, include=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1

trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") # strips the date from its current format
unique_dates <- sort(unique(trip$date)) # this collects unique dates
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) # this maps unique date to the integers, starting at 1
trip$day_number = date_to_number[as.character(trip$date)] # this adds the integer mapping as a column, day_number
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)

# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = sum(tripduration),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data, include = FALSE}
weather = read_csv('weather.csv.xls')

weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF # calculates temperature range for each day

weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") # strips the date from its current format

# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]

weather = weather[,-1] # remove the old date

# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)

df$avg_durations = df$total_durations / df$total_trips

write.csv(df, "cleaned_data.csv", row.names = FALSE)
```


The \texttt{trip} data frame contains 275,091 cases (or rides) and 12 variables describing each ride. The weather data frame contains 689 cases (or days) and 21 variables describing the weather that day. Ultimately, our goal is to join these two data frames. We began by aggregating trip data for each day we have data for. From the \texttt{trip} data frame, we created a new data frame called \texttt{ridership} that aggregates trips by day. At the end of this, \texttt{ridership} has 901 rows (days) and 3 columns (variables): \texttt{count}, \texttt{tripduration}, and \texttt{day\_number}. Because the \texttt{trip} data covers 212 days after the last observation in the \texttt{weather} data, we want to keep only the observations in \texttt{trip} that match the observations in the smaller data frame, \texttt{weather}. We created our final data frame, \texttt{df}, by left joining weather and ridership by \texttt{day\_number}. The final data frame contains 689 rows (days) and 25 columns (variables). The variable names are listed in a table below with brief descriptions.

```{r rtable, echo = FALSE}
# Create a data frame with variable names and descriptions
variable_descriptions <- data.frame(
  Variable = c(
    "Max_Temperature_F", "Mean_Temperature_F", "Min_TemperatureF",
    "Max_Dew_Point_F", "MeanDew_Point_F", "Min_Dewpoint_F",
    "Max_Humidity", "Mean_Humidity", "Min_Humidity",
    "Max_Sea_Level_Pressure_In", "Mean_Sea_Level_Pressure_In", "Min_Sea_Level_Pressure_In",
    "Max_Visibility_Miles", "Mean_Visibility_Miles", "Min_Visibility_Miles",
    "Max_Wind_Speed_MPH", "Mean_Wind_Speed_MPH", "Max_Gust_Speed_MPH",
    "Precipitation_In", "Events", "temp_range", "date", "day_number",
    "total_trips", "total_durations", "average_durations"
  ),
  Description = c(
    "Maximum temperature (°F) recorded that day",
    "Mean temperature (°F) recorded that day",
    "Minimum temperature (°F) recorded that day",
    "Maximum dew point (°F) recorded that day",
    "Mean dew point (°F) recorded that day",
    "Minimum dew point (°F) recorded that day",
    "Maximum humidity (%) recorded that day",
    "Mean humidity (%) recorded that day",
    "Minimum humidity (%) recorded that day",
    "Maximum sea-level pressure in inches recorded that day",
    "Mean sea-level pressure in inches recorded that day",
    "Minimum sea-level pressure in inches recorded that day",
    "Maximum visibility in miles recorded that day",
    "Mean visibility in miles recorded that day",
    "Minimum visibility in miles recorded that day",
    "Maximum wind speed in miles per hour recorded that day",
    "Mean wind speed in miles per hour recorded that day",
    "Maximum gust speed in miles per hour recorded that day",
    "Precipitation in inches recorded that day",
    "Weather events (e.g., Rain, Snow) that occurred that day",
    "Temperature range (Max_Temperature_F - Min_TemperatureF)",
    "Date of the observation",
    "Days since 12 October 2014",
    "Total trips recorded that day",
    "Total duration of all trips recorded that day in seconds",
    "Average ride duration for that day"
  )
)

# Render the table using kable
knitr::kable(variable_descriptions, col.names = c("Variable", "Description"), caption = "Variable Descriptions (689 rows, 25 columns)")

```

Notice that some variables in our data dictionary were created from others:
\begin{itemize}
\item \texttt{temp\_range} was created by the difference: \texttt{Max\_Temperature\_F} - \texttt{Min\_TemperatureF}
\item \texttt{date} is in the format \texttt{"\%m/\%d/\%Y"}, because each trip was originally in the format \texttt{"\%m/\%d/\%Y \%H:\%M"}
\item \texttt{day\_number} are the days since 12 October 2014, the first day of observation
\item \texttt{total\_trips} are the total trips recorded for that day
\item \texttt{total\_durations} are the total durations for all trips that day
\item \texttt{avg\_durations} are the average durations for a trip each day
\end{itemize}


## Selecting the Response Variable
The three candidates for a good response variable were created from the ``trip.csv`` data set, described in the ``ridership`` data frame, and merged into our final data frame: ``total_trips``, ``trip_durations``, and ``avg_durations``. We briefly discuss the merits of each response variable before a quantitative judgement:
\begin{itemize}
\item \texttt{total\_trips} is the most intuitive measure for bike ridership on a given day. It directly answers the question ``How many trips were there?'' for a given day. It gives us a picture of how willing people in the service area were to hop on a bike.
\item \texttt{total\_durations} gives a more complete picture for the ridership on a given day. Once a rider hopped on a bike, how long did they ride before docking it? This gives us a picture of how willing riders in the service area were to stay on their bikes.
\item \texttt{avg\_durations} controls for the interaction between bike ridership and ridership durations. By dividing total ridership over total durations, we understand the willingness of those in the service area to both picking up a bike and keep riding on that bike.
\end{itemize}

The figure below plots daily bike ridership in Seattle, with the total rides taken each day in blue circles and the sum of the durations of the rides taken each day in red triangles. This figure suggests that outliers in total riders tend to coincide with outliers in ride durations. For instance, the day with the highest bike riders -- 941 on Sunday, April 20, 2015 -- was also the day with the second highest sum of ride durations (359.7 hours). It's not clear from lookup what caused bike ridership to be so high on this day; like much of the data we gather from the real world, this result was influenced by many factors that day.



```{r sum stats,include = FALSE}
summary(df$total_trips)
summary(df$total_durations)
summary(df$avg_durations)
```


```{r plot_y, echo = FALSE}

par(mar = c(4, 4, 4, 4) + 0.1)  # Increase the right margin (4th value)

# Set up the plot
plot(df$day_number, df$total_trips, 
     xlab = "Days since 12 October 2014", 
     ylab = "Total Bike Ridership", 
     col = "black", 
     bg = "blue",
     pch = 21, 
     ylim = c(0, max(df$total_trips)))  # Primary Y-axis

# Overlay the second dataset
par(new = TRUE)  # Add a new plot without clearing the previous one
plot(df$day_number, df$total_durations, 
     xlab = "", 
     ylab = "", 
     col = "black", 
     bg = "red",
     pch = 24, 
     axes = FALSE,  # Suppress axes for this plot
     ylim = c(0, max(df$total_durations)))  # Secondary Y-axis scale

# Add the secondary Y-axis
axis(4)  # Right Y-axis in red
mtext("Total Durations (secs)", side = 4, line = 3)  # Label for right Y-axis

# Add a title for the entire plot
title(main = "Daily Bike Share Ridership in Seattle", line = 1, cex.main = 1.25)

# Add a legend to differentiate datasets
legend("topright", legend = c(expression(Sigma ~ "Riders"), expression(Sigma ~ "Durations (secs)")), 
       col = c("black", "black"), 
       pt.bg = c("blue", "red"),
       pch = c(21, 24), 
       bty = "n")  # No box around the legend

```

36 days earlier on Sunday, March 15, 2015 was the second-wettest March day on record in the Puget Sound Region (SOURCE). The rain was so severe that a mudslide occured in Western Seattle. Knowing this, you'd expect March 15 to have been a bad day for cycling. Only 34 trips took place on this day with a combined ride duration of just 6.3 hours. This was the second worst day for cycling behind Sunday, December 27, 2015 with just 30 trips and 4.5 hours. The coincidence between trips and durations explains the flattening of the data -- the decline in variation from the mean -- once we compute the average ride durations per day. We choose to skip a visualization of the average each day to visualizing the normality of the data. 

We note (i) that \texttt{total\_trips} and \texttt{total\_durations} are highly correlated (>0.82), (ii) that average durations is bimodal, total durations is right-skewed, and total trips is roughly normal, and (iii) total durations makes for the easiest interpretation without being transformed. We therefore use \texttt{total\_trips} as our response variable going forward. 


```{r plot_normal_histograms, echo = FALSE}
par(mfrow = c(3, 2),       # Create a 3x2 grid
    mar = c(2, 2, 2, 1),   # Reduce margins around individual plots (bottom, left, top, right)
    oma = c(4, 4, 2, 2))   # Set outer margins for the entire figure

hist(df$total_trips, xlab='', main='Total Trips'); hist(log(df$total_trips), xlab='', main='log(Total Trips)')
hist(df$total_durations, xlab='', main='Total Durations'); hist(log(df$total_durations), xlab='', main='log(Total Durations)')
hist(df$avg_durations, xlab='', main='Average Durations'); hist(log(df$avg_durations), xlab='', main='log(Average Durations)')
```

```{r plot exploratory table, echo = FALSE}
# Create a new data frame with grouped features
compact_correlations <- data.frame(
  Feature = c("Temperature (°F)", "Visibility (miles)", "Dew Point (°F)", "Humidity (%)", "Sea Level Pressure (in)"),
  Min = c(
    round(cor(df$Min_TemperatureF, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Dewpoint_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Mean = c(
    round(cor(df$Mean_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$MeanDew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Max = c(
    round(cor(df$Max_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Dew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  )
)

# Create a table
knitr::kable(compact_correlations, caption = "Correlation between Weather Features and Total Trips")

```
```{r wind cor plots, echo = FALSE, warning = FALSE}

df$Max_Gust_Speed_MPH <- as.numeric(as.character(df$Max_Gust_Speed_MPH))


# Wind plots
par(mfrow=c(1,3)) # Set the layout for 3 plots in a single row
# Left-most plot
par(mar=c(5, 4, 4, 0.5)) # Bottom, Left, Top, Right margins
plot(df$Max_Wind_Speed_MPH, df$total_trips, 
     xlab="Max Wind Speed (MPH)", 
     ylab="Total Rides", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Wind_Speed_MPH)),")"))
# Middle plot 
par(mar=c(5, 2.25, 4, 2.25)) # Reduce left margin
plot(df$Mean_Wind_Speed_MPH, df$total_trips, 
     xlab="Mean Wind Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Mean_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Mean_Wind_Speed_MPH)),")"))
# Right-most plot
par(mar=c(5, 0.5, 4, 4)) # Reduce left margin
plot(df$Max_Gust_Speed_MPH, df$total_trips, 
     xlab="Max Gust Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Gust_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Gust_Speed_MPH)),")"))

```
## Outliers


# Methods/ Analysis
```{r plot predictors, echo=FALSE}
par(mfrow=c(2,2), mai = c(1,0.1,0.1,0.1))
plot(df$Precipitation_In,df$total_trips, xlab = "Precipitation (in)"); abline(reg = lm(df$total_trips ~ df$Precipitation_In), col = 'red')

plot(df$Mean_Temperature_F,df$total_trips, xlab = "Mean Temperature (°F)"); abline(reg = lm(df$total_trips ~ df$Mean_Temperature_F), col = 'red')

plot(df$Mean_Humidity,df$total_trips, xlab = "Mean Humidity (%)"); abline(reg = lm(df$total_trips ~ df$Mean_Humidity), col = 'red')

plot(df$Mean_Wind_Speed_MPH, df$total_trips, xlab = "Mean Wind Speed (MPH)"); abline(reg = lm(df$total_trips ~ df$Mean_Wind_Speed_MPH), col = 'red')
```

```{r plot diag plots, echo=FALSE}
out = lm(total_trips ~ Mean_Temperature_F, data = df)
#par(mfrow=c(2,2))
#plot(out)
#summary(out)
#Adjusted R^2 is 0.5612

outHum = lm(total_trips ~ Mean_Humidity, data = df)
par(mfrow=c(2,2))
plot(outHum)
summary(outHum)
#Diagnostic plots look incredible. Adj R^2 = 0.4611

outRain = lm(total_trips ~ Precipitation_In, data = df)
#par(mfrow=c(2,2))
#plot(outRain)
#summary(outRain)

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data =df)
#par(mfrow=c(2,2))
#plot(outWind)
#summary(outWind)
#Plots bad, R^2 <0.2
```

```{r modelling, include = FALSE}
rainResiduals <- resid(outRain)
windResiduals <- resid(outWind)

variance_rain = lm(abs(windResiduals) ~ Mean_Wind_Speed_MPH, data = df)

rainpredictedvar = predict(variance_rain)

rainweights = 1/(rainpredictedvar^2)

RainWLS <- lm(total_trips ~ Precipitation_In, data = df, weights = rainweights)

#RainWLS did not produce meaningful improvements.

outDew = lm(total_trips ~ MeanDew_Point_F, data = df)

#Great plots R^2 is only 0.2

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data = df)

#Good plots R^2 is a paltry 0.07

windweights <- 1/lm(abs(outWind$residuals)~outWind$fitted.values)$fitted.values^2

WindWLS <- lm(total_trips ~ Mean_Wind_Speed_MPH, data = df, weights = windweights)

#Using WLS on windspeed significantly improves it. The R^2 is 0.7

outRange = lm(total_trips ~ temp_range, data = df)

#Plots have 1 extremely influential point, the R^2 is 0.315
#If we are to use mean temp I think we can ignore this variable (as a basic regressor)

outVisible = lm(total_trips ~ Mean_Visibility_Miles, data = df)

#Plots are mid, R^2 is 0.1313

visresid =  resid(outVisible)

variance_vis = lm(abs(visresid)~Mean_Visibility_Miles, data=df)

vispredictvar = predict(variance_vis)

visweights = 1/(vispredictvar^2)

VisWLS <- lm(total_trips ~ Mean_Visibility_Miles, data = df, weights = visweights)

#Very unimpressive.

outSea = lm(total_trips ~ Mean_Sea_Level_Pressure_In, data = df)

#Criteria for being in full model is that it had a *** significance by itself

fullmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles, data = df)

#Very interestingly Mean temp is not significant. Additionally visibility is far from significant

#Diagnostic plots look very good. R^2 is 0.71

partialmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

partialmodel2 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#Good diagnostics, R^2 is 0.71, all regressors are significant.

#I think that this is the best model.

testmodel = lm(total_trips ~ Mean_Temperature_F + MeanDew_Point_F, data=df)

#R^2 0.63, good diagnostics.

testmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity, data=df)

#R^2 0.6487, good diagnostics

testmodel3 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F, data=df)

#Suddenly temperature is not significant


powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,(df$Precipitation_In+1),(df$Mean_Wind_Speed_MPH+1),df$Mean_Visibility_Miles))

#This fails as powerTransform needs arguments to be strictly positive and the min
#of Precipitation and Wind speed are 0

powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,df$Mean_Visibility_Miles))

#trips: 0.761, Temperature: 0.760, Humidity: 0.67, Dew point 1.1, Visibility 10

#Therefore trips 3/4, temperate 3/4, humidity 2/3, Dew point no change visibility, visibility^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^10

transform_out <- lm(total_trips_trans ~ Mean_Temperature_F_trans + Mean_Humidity_trans + MeanDew_Point_F + Mean_Visibility_Miles_trans, data = df)

#The transformed model is not very impressive. Good diagnostics, R^2 of 0.6484

partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)

#2nd best model

partialmodel4 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F+temp_range, data = df)

fullmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles+Mean_Sea_Level_Pressure_In, data = df)

vif(partialmodel3)

cor(df[, c("Mean_Humidity", "MeanDew_Point_F", "Max_Temperature_F", "temp_range")])

anova(partialmodel,fullmodel2)

df_clean <- na.omit(df)

fullmodel <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Events + Precipitation_In,
                data = df)

alias(fullmodel)

nullmodel <- lm(total_trips ~ 1, data = df)


#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodel))

forwardselectionmodel = lm(total_trips~Max_Temperature_F+Precipitation_In+Mean_Humidity
                           +Mean_Wind_Speed_MPH+Mean_Sea_Level_Pressure_In+Min_Visibility_Miles
                           +Max_Wind_Speed_MPH+Max_Humidity+Min_Dewpoint_F+Mean_Temperature_F, data=df)

#Bad

#stepAIC(fullmodel, direction = "backward")

backwardselectionmodel = lm(total_trips~ Max_Temperature_F+Mean_Temperature_F+Min_Dewpoint_F+Max_Humidity+Mean_Humidity+Mean_Sea_Level_Pressure_In
                            +Max_Wind_Speed_MPH+Mean_Wind_Speed_MPH+Precipitation_In, data=df)

#better but still worse than what we already have

all_subsets <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Events + Precipitation_In, 
                          data = df, nvmax = 10)  # nvmax specifies max number of predictors

# View a summary of the results
summary(all_subsets)

# Extract the summary
subsets_summary <- summary(all_subsets)

# Display key metrics
subsets_summary$adjr2  # Adjusted R^2 for each model
subsets_summary$bic    # BIC for each model
subsets_summary$cp     # Mallow's Cp for each model

best_adj_r2_index <- which.max(subsets_summary$adjr2)
cat("Best model by Adjusted R^2 is Model", best_adj_r2_index, "with Adjusted R^2:", max(subsets_summary$adjr2), "\n")

best_bic_index <- which.min(subsets_summary$bic)
cat("Best model by BIC is Model", best_bic_index, "with BIC:", min(subsets_summary$bic), "\n")

# View variables for the best model (by Adjusted R^2 as an example)
subsets_summary$outmat[best_adj_r2_index, ]

partialmodel5 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#This model does not have issues with multicolinearity

df$Event_Binary <- ifelse(!is.na(df$Events), 1, 0)

eventout = lm(total_trips~Event_Binary, data =df)

fullmodelevent <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Precipitation_In+Event_Binary,
                data = df)

#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodelevent))

#stepAIC(fullmodelevent, direction = "backward")

all_subsetsevent <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Event_Binary + Precipitation_In, 
                          data = df, nvmax = 10)

# Get the summary of the regsubsets output
subset_summary <- summary(all_subsetsevent)

# Extract metrics
rsquared <- subset_summary$rsq
adj_r_squared <- subset_summary$adjr2
bic <- subset_summary$bic
cp <- subset_summary$cp

# View the metrics
print(rsquared)
print(adj_r_squared)
print(bic)
print(cp)

# Best model by BIC (minimum value)
best_bic_model <- which.min(bic)

# Best model by adjusted R^2 (maximum value)
best_adjr2_model <- which.max(adj_r_squared)

# Best model by Cp (closest to the number of predictors)
best_cp_model <- which.min(abs(cp - subset_summary$np))

# Print the results
cat("Best model by BIC:", best_bic_model, "\n")
cat("Best model by Adjusted R^2:", best_adjr2_model, "\n")
cat("Best model by Cp:", best_cp_model, "\n")


# Extract coefficients for the best model (e.g., by BIC)
best_model_coeffs <- coef(all_subsetsevent, id = best_bic_model)

# View the coefficients
print(best_model_coeffs)

#Final model (best model)

modela = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

modelb = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+log(Precipitation_In+0.0001)+log(Mean_Wind_Speed_MPH+0.0001), data = df)

df$PrecipitationDummy <- ifelse(df$Precipitation_In > 0, 1, 0)

modelc = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+PrecipitationDummy+Mean_Wind_Speed_MPH, data = df)

df$EventsDummy <- ifelse(is.na(df$Events), 0, 1)

modeld = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Mean_Wind_Speed_MPH+EventsDummy, data = df)

df$RainDummy = ifelse(grepl("Rain", df$Events, fixed = TRUE), 1, 0)

modele= lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+RainDummy+Mean_Wind_Speed_MPH, data = df)

numeric_vars <- df[sapply(df, is.numeric)]

# Compute the correlation matrix
correlation_table <- cor(numeric_vars, use = "complete.obs")

# View the correlation table
print(correlation_table)


```

# Conclusion/Discussion



```{r, echo=FALSE, include=FALSE}
cor(df$total_durations,df$total_trips)
#Total trips and total durations are only 82.2% correlated

cor(df$total_durations, df$Mean_Temperature_F, use = "complete.obs")

cor(df$total_durations, df$MeanDew_Point_F)


```




## Model Selection

Our model and how we derived it:

The equation for our final regression model is:

  \texttt{total\_trips} = 277.1643 – 4.95(\texttt{Mean\_Humidity}) + 5.90(\texttt{MeanDew\_Point}) – 118.151(\texttt{Precipitation\_In}) – 7.91(\texttt{Mean\_Wind\_Speed}) + 2.94(\texttt{Max\_Temperature})

Each regressor is significant to at least the 0.01 level, and the diagnostic plots are satisfactory. The residual plot moving-average looks flat. The normal quantile plot has few departures from the line. The scale location plot is relatively flat, indicating constant variance across fitted values. 

In order to obtain this model, we first did some exploratory data analysis, plotting total trips against certain variables and obtaining their correlation. We then attemped several basic (single regressor) linear models of total_trips vs some of our variables. The variables that we considered were: mean temperature, mean humidity, mean dew point, precipitation in inches, mean wind speed, mean miles of visibility, mean sea level pressure,  temperature range, and max temperature. These were the original variables we considered on account of the fact that they seemed to have some relationship with total trips based on plots and correlation, and made sense to us as predictors of ridership from an intuitive perspective. After trying several single variable models, we found that few of them had very high R^2, with a  notable exception that a weighted least squares model of wind speed high worked very well. We then decided to make a “full” model with all of the variables above, except only using mean temperature instead of temperature range and max temperature as 1. mean and max were extremely highly correlated (>97%) and 2. we thought two temperature variables would be redundant. The output of the full model showed that Mean temperature, mean visibility miles, and mean sea level pressure did not have coefficients that were significantly different from zero. We made a model dropping these parameters and then did an anova (partial f) between the two models to see if we could justifiably drop them and it showed that we could. At this point all of the predictors were significant and the adjusted R^2 was 0.7095. However, we got the idea to try adding temperature range or max temperature to this model. Including temperature range did not improve the R^2, and it was not significant in the model, however, including max temperature did improve the adjusted R^2 and the regressor was also significant. We decided that this would be our final model. Each regressor is significant at at least the 0.01 level, and the diagnostic plots look good. The adjusted R^2 is 0.712. 
We also tried to use powerTransformations, but it only worked for some of the variables as others did not have strictly positive values. For the variables that did successfully power transform, the adjusted R^2 of the subsequent model was not greatly improved and few of the regressors were significant. 
Our final model has some nice properties, in that the diagnostic plots show that it satisfies the assumptions for linear regression well, it is perfectly basic in terms of transformations, and partly on account of that, it is not too difficult to interpret. 
In terms of interpretation, our model predicts that holding all else equal, every 1% increase in average humidity will lead the total number of bike trips to decrease by 4.95. It predicts that holding all else equal, for every 1 degree increase in the average dew point (Fahrenheit for this and all future mentions of degrees) Seattle will see a drop in bike trips of 5.90.  Our model predicts that all else equal, for every 1 extra inch of precipitation, total bike rides will drop by 118. It also predicts that holding all else equal, a 1 mile per hour increase in the average wind speed will decrease total bike trips by 7.91. Lastly our model predicts that holding all else equal, a 1 degree increase in the maximum temperature will increase the number of bike trips by 2.94. 

```{r final model, echo = FALSE}
partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)

#regression summary of the final model
summary(partialmodel3)

# diagnostic plots
par(mfrow=c(2,2))
plot(partialmodel3)

# pairs plot
pairs(~Mean_Humidity + MeanDew_Point_F + Precipitation_In + Mean_Wind_Speed_MPH + Max_Temperature_F, data = df)
```

## Cross Validation

# Conclusion
Our work here suggests a path forward for bike share systems looking to bolster their operations and planning with weather data. However, Seattle is a city with temperate weather/climate. These results are not readily generalizable to all cities because when its too hot, people will also not ride bike!



```{r mapping copy, eval=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1
# strips the date from its current format
trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") 
unique_dates <- sort(unique(trip$date)) # this collects unique dates
# this maps unique date to the integers, starting at 1
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) 
# this adds the integer mapping as a column, day_number
trip$day_number = date_to_number[as.character(trip$date)] 
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)

# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = sum(tripduration),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data copy, eval = FALSE}
weather = read_csv('weather.csv.xls')
# calculates temperature range for each day
weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF 
# strips the date from its current format
weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") 
# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]
weather = weather[,-1] # remove the old date
# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)
df$avg_durations = df$total_durations / df$total_trips
```

