---
title: "Predicting Bike Share Ridership based on Climate Data in Seattle"
author: "Joey Rodriguez and Daniel Bhatti"
date: "2024-11-22"
output:
  pdf_document: default
  html_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#imports 
library(readr)
library(dplyr)
library(car)
library(tinytex)
library(ggplot2)
library(patchwork)
library(zoo)
library(knitr)
library(MASS)
library(olsrr)
library(leaps)
library(lubridate)
```

# Introduction
Bike share has launched in many U.S. cities since its introduction in Washington, D.C. in 2010 (1). One iteration of bike share  was \emph{Pronto!} in downtown Seattle, Washington. From 2014 to 2017, 500 \emph{Pronto!} bikes operated across 54 stations on the ithsmus. The City of Seattle partnered with Socratica to collect system data during the operating window and made it publicly available via its open data platform. \emph{Pronto!} fell short of the success realized by other similar schemes in the U.S. like Capital Bikeshare, Philly’s Indego, and NYC’s CitiBike. Researchers have used this system data to conduct a post-mortem analysis as dockless providers like \emph{Lime Micromobility} filled the void left by \emph{Pronto!} (2). 

In this brief paper, we  investigate the relationship between weather in the service area and daily ridership. In particular, we predict daily ridership based on weather data and time of year. After cleaning the data and analyzing candidates for the response variable, we took a stepwise approach to fitting a model. At first we considered only weather-related predictors, but we strengthened our model by adding temporal predictors. We considered various aspects when comparing models such as diagnostic plots, summary statistics, multicolinearity, systematic variable selection, ANOVA and partial F-test to compare satisfactory models. For our final model, we consider interpretations, limitations, and extensions worth thorough investigation. 


```{r, include=FALSE, fig.cap = "PRETTY FIGURE", out.width = "50%"}
knitr::include_graphics("pronto.png")
```

# Exploratory Data Analysis
## Data Cleaning
The data \texttt{trip.csv} and \texttt{weather.csv.xls} were downloaded from Kaggle (3). The \texttt{trip} data frame contains 275,091 cases (or rides) and 12 variables describing each ride. These data were collected over 901 days from 13 October 2014 to 31 March 2017. The relevant variables from the original 12 in this dataset are \texttt{start\_time} (day and time trip started, in PST) and \texttt{trip\_duration} (time of trip in seconds). The \texttt{weather} data frame contains 689 cases (or days) and 21 variables describing the weather that day. These data were collected from 13 October 2014 to 31 August 2016, or 689 days. Notice that the dates covered by the \texttt{weather} data set are a proper subset of the dates covered by the \texttt{trip} data set. 

We began by aggregating trip data for each day we have data for. From the \texttt{trip} data frame, we created a new data frame called \texttt{ridership} that aggregates trips by day. At the end of this, \texttt{ridership} has 901 rows (days) and 3 columns (variables): \texttt{count}, \texttt{tripduration}, and \texttt{day\_number}. Because the \texttt{trip} data covers 212 days after the last observation in the \texttt{weather} data, we want to keep only the observations in \texttt{trip} that match the observations in the smaller data frame, \texttt{weather}. We created our final data frame, \texttt{df}, by left-joining weather and ridership by \texttt{day\_number}. The final data frame contains 689 rows (days) and 29 columns (variables). The variable names are listed in the table below with brief descriptions.

```{r mapping, include=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1

trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") # strips the date from its current format
unique_dates <- sort(unique(trip$date)) # this collects unique dates
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) # this maps unique date to the integers, starting at 1
trip$day_number = date_to_number[as.character(trip$date)] # this adds the integer mapping as a column, day_number
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)

# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = round(sum(tripduration),1),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data, include = FALSE}
weather = read_csv('weather.csv.xls')
# calculates temperature range for each day
weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF 
# strips the date from its current format
weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") 
# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]
weather = weather[,-1] # remove the old date
# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)
df$avg_durations = round(df$total_durations / df$total_trips, 1)
write.csv(df, "cleaned_data.csv", row.names = FALSE)
```

```{r add season variables, echo = FALSE}
df$weekday_weekend <- ifelse(weekdays(df$date) %in% c("Saturday", "Sunday"),1,0)
# Define a function to classify seasons based on actual start dates
get_season <- function(date) {
  year <- lubridate::year(date)
  spring_start <- as.Date(paste0(year, "-03-20"))
  summer_start <- as.Date(paste0(year, "-06-21"))
  fall_start   <- as.Date(paste0(year, "-09-22"))
  winter_start <- as.Date(paste0(year, "-12-21"))
  ifelse(date >= spring_start & date < summer_start, 0,  # Spring
  ifelse(date >= summer_start & date < fall_start,   1,  # Summer
  ifelse(date >= fall_start & date < winter_start,   2,  # Fall
  3)))  # Winter
}
# Apply the function to the 'date' variable
df$season <- sapply(df$date, get_season)
# Create fall/winter dummy: 1 if Fall or Winter, 0 otherwise
df$fall_winter <- ifelse(df$season %in% c(2, 3), 1, 0)
```

```{r rtable, echo = FALSE}
# Create a data frame with variable names and descriptions
variable_descriptions <- data.frame(
  Variable = c(
    "Max_Temperature_F", "Mean_Temperature_F", "Min_TemperatureF",
    "Max_Dew_Point_F", "MeanDew_Point_F", "Min_Dewpoint_F",
    "Max_Humidity", "Mean_Humidity", "Min_Humidity",
    "Max_Sea_Level_Pressure_In", "Mean_Sea_Level_Pressure_In", "Min_Sea_Level_Pressure_In",
    "Max_Visibility_Miles", "Mean_Visibility_Miles", "Min_Visibility_Miles",
    "Max_Wind_Speed_MPH", "Mean_Wind_Speed_MPH", "Max_Gust_Speed_MPH",
    "Precipitation_In", "Events", "temp_range", "date", "day_number",
    "total_trips", "total_durations", "average_durations", "weekday_weekend",
    "season", "fall_winter"
  ),
  Description = c(
    "Maximum temperature (°F)",
    "Mean temperature (°F)",
    "Minimum temperature (°F)",
    "Maximum dew point (°F)",
    "Mean dew point (°F)",
    "Minimum dew point (°F)",
    "Maximum humidity (%)",
    "Mean humidity (%)",
    "Minimum humidity (%)",
    "Maximum sea-level pressure (inches Hg)",
    "Mean sea-level pressure (inches Hg)",
    "Minimum sea-level pressure (inches Hg)",
    "Maximum visibility (miles)",
    "Mean visibility (miles)",
    "Minimum visibility (miles)",
    "Maximum wind speed (MPH)",
    "Mean wind speed (MPH)",
    "Maximum gust speed (MPH)",
    "Precipitation (inches)",
    "Weather events (e.g., Rain, Snow)",
    "Temperature range (°F)",
    "Date of the observation (%m/%d/%Y)",
    "Days since 12 October 2014",
    "Count of total trips",
    "Sum of total duration for all trips (seconds)",
    "Average ride duration (seconds)",
    "Encodes weekends: 1 if Saturday or Sunday, 0 otherwise",
    "Encodes seasons: 0 if Spring, 1 if Summer, 2 if Fall, 3 if Winter", 
    "Encodes wet season: 1 if Fall or Winter, 0 if Summer or Spring"
  )
)
# Render the table using kable
knitr::kable(variable_descriptions, col.names = c("Variable", "Description"), caption = "Variable Descriptions (689 days, 29 variables)")
```

Notice that nine variables in our data dictionary were created from other variables:
\begin{itemize}
\item \texttt{temp\_range} is computed from the difference: \texttt{Max\_Temperature\_F} - \texttt{Min\_TemperatureF} 
\item \texttt{date} strips the day/ month/ year \texttt{"\%m/\%d/\%Y"} from the full \texttt{starttime} \texttt{"\%m/\%d/\%Y \%H:\%M"}
\item \texttt{day\_number} are the days beginning 13 October 2014, the first day of observation
\item \texttt{total\_trips} are the total trips recorded for each day
\item \texttt{total\_durations} are the total durations for all trips each day
\item \texttt{avg\_durations} was computed from the difference: \texttt{total\_durations} / \texttt{total\_trips}
\item \texttt{weekday\_weekend} = 1 if \texttt{date} was a Saturday or Sunday and 0 otherwise.
\item \texttt{season} = 0 if \texttt{date} in Spring, 1 if Summer, 2 if Fall, 3 if Winter according the the summer and winter solstices and the spring and fall equinoxes in the Northern hemisphere.
\item \texttt{fall\_winter} was created based on whether the \texttt{season} was Fall or Winter, which roughly coincides with the wet season in the Puget Sound Region from October to April (SOURCE).
\end{itemize}


## Understanding Outliers

The figure below plots daily bike ridership in Seattle, with the total rides taken each day in blue circles and the sum of the durations of the rides taken each day in red triangles. This figure suggests that outliers in total riders tend to coincide with outliers in ride durations. For instance, the day with the highest bike riders -- 941 on Monday, April 20, 2015 -- was also the day with the second highest sum of ride durations (359.7 hours). This is day 190, the largest outlier in the data set. It's not clear from look-up what caused bike ridership to be so high on this day; like much of the data gathered from the real world, many factors likely contributed to high ridership on this day.

36 days earlier on Sunday, March 15, 2015 was the second-wettest March day on record in the Puget Sound Region (SOURCE). The rain was so severe that a mudslide occurred in Western Seattle. Knowing this, you'd expect March 15 to have been a bad day for cycling, and you'd be right: only 34 trips took place on this day with a combined ride duration of just 6.3 hours. This was the second worst day for cycling behind Sunday, December 27, 2015 with just 30 trips totaling 4.5 hours. This also happens to be the greatest leverage point by far (0.144). The coincidence between trips and durations explains the flattening of the data -- the decrease in variation from the mean -- observed in \texttt{avg\_durations}.


```{r sum stats,include = FALSE}
summary(df$total_trips)
summary(df$total_durations)
summary(df$avg_durations)
```


```{r plot_y, echo = FALSE, fig.cap = "Daily Bike Share Ridership and Durations in Seattle."}

par(mar = c(4, 4, 4, 4) + 0.1)  # Increase the right margin (4th value)

# Set up the plot
plot(df$day_number, df$total_trips, 
     xlab = "Days since 12 October 2014", 
     ylab = "Total Bike Ridership", 
     col = "black", 
     bg = "blue",
     pch = 21, 
     ylim = c(0, max(df$total_trips)))  # Primary Y-axis

# Overlay the second data set
par(new = TRUE)  # Add a new plot without clearing the previous one
plot(df$day_number, df$total_durations, 
     xlab = "", 
     ylab = "", 
     col = "black", 
     bg = "red",
     pch = 24, 
     axes = FALSE,  # Suppress axes for this plot
     ylim = c(0, max(df$total_durations)))  # Secondary Y-axis scale

# Add the secondary Y-axis
axis(4)  # Right Y-axis in red
mtext("Total Durations (secs)", side = 4, line = 3)  # Label for right Y-axis

# Add a title for the entire plot
title(main = "Daily Bike Share Ridership in Seattle", line = 1, cex.main = 1.25)

# Add a legend to differentiate data sets
legend("topright", legend = c(expression(Sigma ~ "Riders"), expression(Sigma ~ "Durations (secs)")), 
       col = c("black", "black"), 
       pt.bg = c("blue", "red"),
       pch = c(21, 24), 
       bty = "n")  # No box around the legend

```

# Methods

## Selecting the Response Variable
The three candidates for our response variable were created from the ``trip.csv`` data set, described in the ``ridership`` data frame, and merged into our final data frame: ``total_trips``, ``trip_durations``, and ``avg_durations``. We briefly discuss the strengths of each response variable before a quantitative judgement:
\begin{itemize}
\item \texttt{total\_trips} is the most intuitive measure for bike ridership on a given day. It directly answers the question ``How many trips were there?'' for a given day. It gives us a picture of how willing people in the service area were to pick a bike.
\item \texttt{total\_durations} gives a more complete picture for the ridership on a given day. Once a rider picked a bike, how long did they ride before docking it? This gives us a picture of how willing riders in the service area were to stay on their bikes once they mounted.
\item \texttt{avg\_durations} controls for the interaction between bike ridership and ridership durations. By dividing total ridership over total durations, we understand the willingness of those in the service area to picking up a bike \emph{and} staying on it.
\end{itemize}


```{r plot_normal_histograms, echo = FALSE, fig.cap = "Variables Measuring Bike Share Ridership in Seattle"}
par(mfrow = c(3, 2),       # Create a 3x2 grid
    mar = c(2, 2, 2, 1),   # Reduce margins around individual plots (bottom, left, top, right)
    oma = c(4, 4, 2, 2))   # Set outer margins for the entire figure

hist(df$total_trips, xlab='', main='Total Trips'); hist(log(df$total_trips), xlab='', main='log(Total Trips)')
hist(df$total_durations, xlab='', main='Total Durations'); hist(log(df$total_durations), xlab='', main='log(Total Durations)')
hist(df$avg_durations, xlab='', main='Average Durations'); hist(log(df$avg_durations), xlab='', main='log(Average Durations)')
```

Note that \texttt{total\_trips} and \texttt{total\_durations} are highly correlated (>0.82). This is consistent with our discussion on outliers. Considering the shape and spread of the distributions, note that Average Durations is bimodal, Total Durations is right-skewed, and Total Trips is roughly normal. It's clear that Total Durations makes for the easiest interpretation without being transformed. Based on visual inspection, its the most normal appearing on the six distributions. We therefore use \texttt{total\_trips} as our response variable going forward. 

## Selecting the Predictors

```{r plot exploratory table, echo = FALSE}
# Create a new data frame with grouped features
compact_correlations <- data.frame(
  Feature = c("Temperature (°F)", "Visibility (miles)", "Dew Point (°F)", "Humidity (%)", "Sea Level Pressure (in)"),
  Min = c(
    round(cor(df$Min_TemperatureF, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Dewpoint_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Min_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Mean = c(
    round(cor(df$Mean_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$MeanDew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Mean_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  ),
  Max = c(
    round(cor(df$Max_Temperature_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Visibility_Miles, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Dew_Point_F, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Humidity, df$total_trips, use = 'na.or.complete'), 3),
    round(cor(df$Max_Sea_Level_Pressure_In, df$total_trips, use = 'na.or.complete'), 3)
  )
)

# Create a table
knitr::kable(compact_correlations, caption = "Correlation between Weather Features and Total Trips")

```

```{r wind plots, echo = FALSE, warning=FALSE, fig.cap = "Effect of Wind Speed on Bike Share Ridership in Seattle."}
df$Max_Gust_Speed_MPH <- as.numeric(as.character(df$Max_Gust_Speed_MPH))
# Wind plots
par(mfrow=c(1,3)) # Set the layout for 3 plots in a single row
# Left-most plot
par(mar=c(5, 4, 4, 0.5)) # Bottom, Left, Top, Right margins
plot(df$Max_Wind_Speed_MPH, df$total_trips, 
     xlab="Max Wind Speed (MPH)", 
     ylab="Total Rides", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Wind_Speed_MPH)),")"))
# Middle plot 
par(mar=c(5, 2.25, 4, 2.25)) # Reduce left margin
plot(df$Mean_Wind_Speed_MPH, df$total_trips, 
     xlab="Mean Wind Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Mean_Wind_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Mean_Wind_Speed_MPH)),")"))
# Right-most plot
par(mar=c(5, 0.5, 4, 4)) # Reduce left margin
plot(df$Max_Gust_Speed_MPH, df$total_trips, 
     xlab="Max Gust Speed (MPH)", 
     ylab="", 
     sub = paste0("cor(x,y) = ", 
            round(cor(df$Max_Gust_Speed_MPH, df$total_trips, use = 'na.or.complete'), 3), 
            " (n = ", 689 - sum(is.na(df$Max_Gust_Speed_MPH)),")"))

```

For each of the continuous variables with recorded min, mean, and max  –- Visibility, Temperature, Dew Point, Humidity, and Sea Level Pressure -- we calculate their correlations with the response Total Trips. By feature, it turns out that Max Temperature, Min Visibility, Mean Dew Point, Mean Humidity, and Min Sea Level Pressure have the highest correlation with Total Trips (Table 2). Total Trips' high correlation with Max Temperature may be explained by the temperature at midday, when the temperature is usually highest and when people are prone to be out biking between commutes and leisure. We added each of these highest-correlation variables with the response to our baseline model. 

Wind speed is a special case with variables Max Wind Speed, Mean Wind Speed, and Max Gust Speed. Though \texttt{Max\_Gust\_Speed\_MPH} had higher correlation with Total Trips than either \texttt{Max\_Wind\_Speed\_MPH} or \texttt{Mean\_Wind\_Speed\_MPH}, it also had 410 missing values. We opted to add \texttt{Mean\_Wind\_Speed\_MPH} which had the next-highest correlation with Total Trips (Figure 3). We also added Precipitation_In (a zero-inflated continuous variable) and the descriptive variable Events. 

We utilized Events in two ways. First we set it such that Event was a dummy variable representing whether there was an event that day. Events included fog, fog and rain, rain, rain and snow, rain and thunderstorm, and snow. There were 328 days where there were events and 361 without. The second way we utilized Events was by making it a dummy based on whether there was rain or snow on that day (as we might not expect a strong effect from fog). 



## Model Selection

Our baseline model based on initial findings from exploratory data analysis is:

  

$$
\begin{aligned}
\texttt{total\_trips}_i^{base} &= \beta_{i0} + \beta_{i1} \texttt{Mean\_Humidity}_i + \beta_{i2} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i3} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i4} \texttt{Max\_Temperature\_F}_i + \beta_{i5} \texttt{Min\_Visibility\_Miles}_i \\
&+ \beta_{i6} \texttt{Min\_Sea\_Level\_Pressure\_In}_i + \beta_{i7} \texttt{Precipitation\_In}_i + \beta_{i8} \texttt{Events}_i \\
\end{aligned}
$$



We removed \texttt{Min\_Visibility\_Miles}, \texttt{Min\_Sea\_Level\_Pressure\_In} and \texttt{Events} because they were insignificant predictors for Total Trips Our full model at this point ($RSE$ = 83.09, $R^2$ = 0.7141, $R^2_{adj}$= 0.712, \emph{all terms significant to} 0.01) is:

$$
\begin{aligned}
\texttt{total\_trips}_i^{full} &= \beta_{i0} + \beta_{i1} \texttt{Max\_Temperature\_F}_i + \beta_{i2} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i3} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i4} \texttt{Mean\_Humidity}_i + \beta_{i5} \texttt{Precipitation\_In}_i \\
\end{aligned}
$$
Cursory transformations failed to produce superior models. Application of a power transform produced coefficients that were mostly close to one. Both forwards and backwards selection failed to produce a superior model, although they selected models similar to ours. We discovered a partial model ($RSE$ = 83.45, $R^2$ = 0.7112, $R^2_{adj}$= 0.7095, \emph{all terms infinitesimal}) with satisfactory summary statistics and diagnostic plots. The partial model is just the full model with the \texttt{Max\_Temperature\_F} term dropped.

$$
\begin{aligned}
\texttt{total\_trips}_i^{part} &= \beta_{i0} + \beta_{i1} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i2} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i3} \texttt{Mean\_Humidity}_i + \beta_{i4} \texttt{Precipitation\_In}_i \\
\end{aligned}
$$

Based on ANOVA / Partial $F$-test, the full model provides a better fit to the observed data over the partial model ($F$ = 6.9579, $p$ = 0.0085). However, Variance Inflation Factor ($VIF$) -- measuring multicolinearity between the predictors and Total Trips -- is abnormally high for \texttt{Mean\_Humidity} ($VIF$ = 9.05), \texttt{MeanDew\_Point\_F} ($VIF$ = 10.68), and \texttt{Max\_Temperature\_F} ($VIF$ = 19.09) in the full model. 

It turns out that \texttt{Max\_Temperature\_F} has high correlation with \texttt{MeanDew\_Point\_F} (0.72) and \texttt{Mean\_Humidity} (-0.67). The results from $VIF$ analysis makes clear the confounding results in step-wise selection; In the forwards case, \texttt{Max\_Temperature\_F} makes the best 1-predictor model and it is never eliminated in successive model. The partial model has VIF values near one. Thus, the partial model is the better model.

Finally, we strengthen our model by considering temporal variation. Adding the dummy variable \texttt{weekday\_weekend} accounts for weekly changes in ridership based on commute pattern changes in the downtown for leisure, work and school. Adding the variable \texttt{season} accounts for changes in ridership based on the solstices and equinoxes while adding \texttt{fall\_winter} accounts for changes in ridership based only on the solstices:
$$
\begin{aligned}
\texttt{total\_trips}_i^{temp} &= \beta_{i0} + \beta_{i1} \texttt{MeanDew\_Point\_F}_i \\
&+ \beta_{i2} \texttt{Mean\_Wind\_Speed\_MPH}_i + \beta_{i3} \texttt{Mean\_Humidity}_i + \beta_{i4} \texttt{Precipitation\_In}_i \\
&+ \beta_{i5} \texttt{weekday\_weekend}_i + \beta_{i6} \texttt{season}_i + \beta_{i7} \texttt{fall\_winter}_i \\
\end{aligned}
$$
Considering both the high correlation between variables \texttt{season} and \texttt{fall\_winter} and the insignificance of the \texttt{season} variable, we opt to drop this variable from our final model:

$$
\begin{aligned}
\texttt{total\_trips}_i^{fin} &= 485.40 + 8.10 \texttt{MeanDew\_Point\_F}_i \\
&- 8.85 \texttt{Mean\_Wind\_Speed\_MPH}_i - 6.20 \texttt{Mean\_Humidity}_i - 103.49 \texttt{Precipitation\_In}_i \\
&- 50.23 \texttt{weekday\_weekend}_i - 34.11 \texttt{fall\_winter}_i \\
\end{aligned}
$$


```{r plot diag plots, echo=FALSE}
out = lm(total_trips ~ Mean_Temperature_F, data = df)
#par(mfrow=c(2,2))
#plot(out)
#summary(out)
#Adjusted R^2 is 0.5612

outHum = lm(total_trips ~ Mean_Humidity, data = df)
#par(mfrow=c(2,2))
#plot(outHum)
#summary(outHum)
#Diagnostic plots look incredible. Adj R^2 = 0.4611

outRain = lm(total_trips ~ Precipitation_In, data = df)
#par(mfrow=c(2,2))
#plot(outRain)
#summary(outRain)

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data =df)
#par(mfrow=c(2,2))
#plot(outWind)
#summary(outWind)
#Plots bad, R^2 <0.2
```

```{r modelling, include = FALSE}
rainResiduals <- resid(outRain)
windResiduals <- resid(outWind)

variance_rain = lm(abs(windResiduals) ~ Mean_Wind_Speed_MPH, data = df)

rainpredictedvar = predict(variance_rain)

rainweights = 1/(rainpredictedvar^2)

RainWLS <- lm(total_trips ~ Precipitation_In, data = df, weights = rainweights)

#RainWLS did not produce meaningful improvements.

outDew = lm(total_trips ~ MeanDew_Point_F, data = df)

#Great plots R^2 is only 0.2

outWind = lm(total_trips ~ Mean_Wind_Speed_MPH, data = df)

#Good plots R^2 is a paltry 0.07

windweights <- 1/lm(abs(outWind$residuals)~outWind$fitted.values)$fitted.values^2

WindWLS <- lm(total_trips ~ Mean_Wind_Speed_MPH, data = df, weights = windweights)

#Using WLS on windspeed significantly improves it. The R^2 is 0.7

outRange = lm(total_trips ~ temp_range, data = df)

#Plots have 1 extremely influential point, the R^2 is 0.315
#If we are to use mean temp I think we can ignore this variable (as a basic regressor)

outVisible = lm(total_trips ~ Mean_Visibility_Miles, data = df)

#Plots are mid, R^2 is 0.1313

visresid =  resid(outVisible)

variance_vis = lm(abs(visresid)~Mean_Visibility_Miles, data=df)

vispredictvar = predict(variance_vis)

visweights = 1/(vispredictvar^2)

VisWLS <- lm(total_trips ~ Mean_Visibility_Miles, data = df, weights = visweights)

#Very unimpressive.

outSea = lm(total_trips ~ Mean_Sea_Level_Pressure_In, data = df)

#Criteria for being in full model is that it had a *** significance by itself

fullmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles, data = df)

#Very interestingly Mean temp is not significant. Additionally visibility is far from significant

#Diagnostic plots look very good. R^2 is 0.71

partialmodel = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

partialmodel2 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#Good diagnostics, R^2 is 0.71, all regressors are significant.

#I think that this is the best model.

testmodel = lm(total_trips ~ Mean_Temperature_F + MeanDew_Point_F, data=df)

#R^2 0.63, good diagnostics.

testmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity, data=df)

#R^2 0.6487, good diagnostics

testmodel3 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F, data=df)

#Suddenly temperature is not significant


powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,(df$Precipitation_In+1),(df$Mean_Wind_Speed_MPH+1),df$Mean_Visibility_Miles))

#This fails as powerTransform needs arguments to be strictly positive and the min
#of Precipitation and Wind speed are 0

powerTransform(cbind(df$total_trips,df$Mean_Temperature_F,df$Mean_Humidity,df$MeanDew_Point_F,df$Mean_Visibility_Miles))

#trips: 0.761, Temperature: 0.760, Humidity: 0.67, Dew point 1.1, Visibility 10

#Therefore trips 3/4, temperate 3/4, humidity 2/3, Dew point no change visibility, visibility^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^2

df$total_trips_trans <- df$total_trips^(3/4)
df$Mean_Temperature_F_trans <- df$Mean_Temperature_F^(3/4)
df$Mean_Humidity_trans <- df$Mean_Humidity^(2/3)
df$Mean_Visibility_Miles_trans <- df$Mean_Visibility_Miles^10

transform_out <- lm(total_trips_trans ~ Mean_Temperature_F_trans + Mean_Humidity_trans + MeanDew_Point_F + Mean_Visibility_Miles_trans, data = df)

#The transformed model is not very impressive. Good diagnostics, R^2 of 0.6484

partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)

#2nd best model

partialmodel4 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F+temp_range, data = df)

fullmodel2 = lm(total_trips ~ Mean_Temperature_F + Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Mean_Visibility_Miles+Mean_Sea_Level_Pressure_In, data = df)

vif(partialmodel3)

cor(df[, c("Mean_Humidity", "MeanDew_Point_F", "Max_Temperature_F", "temp_range")])

anova(partialmodel,fullmodel2)

df_clean <- na.omit(df)

fullmodel <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Events + Precipitation_In,
                data = df)

alias(fullmodel)

nullmodel <- lm(total_trips ~ 1, data = df)


#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodel))

forwardselectionmodel = lm(total_trips~Max_Temperature_F+Precipitation_In+Mean_Humidity
                           +Mean_Wind_Speed_MPH+Mean_Sea_Level_Pressure_In+Min_Visibility_Miles
                           +Max_Wind_Speed_MPH+Max_Humidity+Min_Dewpoint_F+Mean_Temperature_F, data=df)

#Bad

#stepAIC(fullmodel, direction = "backward")

backwardselectionmodel = lm(total_trips~ Max_Temperature_F+Mean_Temperature_F+Min_Dewpoint_F+Max_Humidity+Mean_Humidity+Mean_Sea_Level_Pressure_In
                            +Max_Wind_Speed_MPH+Mean_Wind_Speed_MPH+Precipitation_In, data=df)

#better but still worse than what we already have

all_subsets <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Events + Precipitation_In, 
                          data = df, nvmax = 10)  # nvmax specifies max number of predictors

# View a summary of the results
summary(all_subsets)

# Extract the summary
subsets_summary <- summary(all_subsets)

# Display key metrics
subsets_summary$adjr2  # Adjusted R^2 for each model
subsets_summary$bic    # BIC for each model
subsets_summary$cp     # Mallow's Cp for each model

best_adj_r2_index <- which.max(subsets_summary$adjr2)
cat("Best model by Adjusted R^2 is Model", best_adj_r2_index, "with Adjusted R^2:", max(subsets_summary$adjr2), "\n")

best_bic_index <- which.min(subsets_summary$bic)
cat("Best model by BIC is Model", best_bic_index, "with BIC:", min(subsets_summary$bic), "\n")

# View variables for the best model (by Adjusted R^2 as an example)
subsets_summary$outmat[best_adj_r2_index, ]

partialmodel5 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

#This model does not have issues with multicolinearity

df$Event_Binary <- ifelse(!is.na(df$Events), 1, 0)

eventout = lm(total_trips~Event_Binary, data =df)

fullmodelevent <- lm(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + Max_Dew_Point_F +
                  MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity + Mean_Humidity + Min_Humidity +
                  Max_Sea_Level_Pressure_In + Mean_Sea_Level_Pressure_In +
                  Min_Sea_Level_Pressure_In + Max_Visibility_Miles + Mean_Visibility_Miles +
                  Min_Visibility_Miles + Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH +
                  Max_Gust_Speed_MPH + Precipitation_In+Event_Binary,
                data = df)

#stepAIC(nullmodel, direction = "forward", scope = list(lower = nullmodel, upper = fullmodelevent))

#stepAIC(fullmodelevent, direction = "backward")

all_subsetsevent <- regsubsets(total_trips ~ Max_Temperature_F + Mean_Temperature_F + Min_TemperatureF + 
                            Max_Dew_Point_F + MeanDew_Point_F + Min_Dewpoint_F + Max_Humidity +
                            Mean_Humidity + Min_Humidity + Max_Sea_Level_Pressure_In + 
                            Mean_Sea_Level_Pressure_In + Min_Sea_Level_Pressure_In + 
                            Max_Visibility_Miles + Mean_Visibility_Miles + Min_Visibility_Miles +
                            Max_Wind_Speed_MPH + Mean_Wind_Speed_MPH + Max_Gust_Speed_MPH + 
                            Event_Binary + Precipitation_In, 
                          data = df, nvmax = 10)

# Get the summary of the regsubsets output
subset_summary <- summary(all_subsetsevent)

# Extract metrics
rsquared <- subset_summary$rsq
adj_r_squared <- subset_summary$adjr2
bic <- subset_summary$bic
cp <- subset_summary$cp

# View the metrics
print(rsquared)
print(adj_r_squared)
print(bic)
print(cp)

# Best model by BIC (minimum value)
best_bic_model <- which.min(bic)

# Best model by adjusted R^2 (maximum value)
best_adjr2_model <- which.max(adj_r_squared)

# Best model by Cp (closest to the number of predictors)
best_cp_model <- which.min(abs(cp - subset_summary$np))

# Print the results
cat("Best model by BIC:", best_bic_model, "\n")
cat("Best model by Adjusted R^2:", best_adjr2_model, "\n")
cat("Best model by Cp:", best_cp_model, "\n")


# Extract coefficients for the best model (e.g., by BIC)
best_model_coeffs <- coef(all_subsetsevent, id = best_bic_model)

# View the coefficients
print(best_model_coeffs)

#Final model (best model)

modela = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH, data = df)

modelb = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+log(Precipitation_In+0.0001)+log(Mean_Wind_Speed_MPH+0.0001), data = df)

df$PrecipitationDummy <- ifelse(df$Precipitation_In > 0, 1, 0)

modelc = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+PrecipitationDummy+Mean_Wind_Speed_MPH, data = df)

df$EventsDummy <- ifelse(is.na(df$Events), 0, 1)

modeld = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Mean_Wind_Speed_MPH+EventsDummy, data = df)

df$RainDummy = ifelse(grepl("Rain", df$Events, fixed = TRUE), 1, 0)

modele= lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+RainDummy+Mean_Wind_Speed_MPH, data = df)

numeric_vars <- df[sapply(df, is.numeric)]

# Compute the correlation matrix
correlation_table <- cor(numeric_vars, use = "complete.obs")

# View the correlation table
print(correlation_table)



outggg = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+factor(season)+weekday_weekend, data = df)

#actual last model

veryfinalmodel = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+fall_winter+weekday_weekend, data = df)


```

```{r very final model, echo = FALSE}
partialmodel3 = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+Max_Temperature_F, data = df)



veryfinalmodel = lm(total_trips ~ Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+fall_winter+weekday_weekend, data = df)

#summary(veryfinalmodel)

#regression summary of the final model
#summary(partialmodel3)

# diagnostic plots
#par(mfrow=c(2,2))
#plot(veryfinalmodel)

# pairs plot
#pairs(~Mean_Humidity + MeanDew_Point_F + #Precipitation_In + Mean_Wind_Speed_MPH + #Max_Temperature_F, data = df)

```

```{r train and test, echo=FALSE}

set.seed(1111)

train_indices <- sample(1:nrow(df), 0.8 * nrow(df))  
#80% for training
train_data <- df[train_indices, ]
test_data <- df[-train_indices, ]

veryfinalmodeltest = lm(total_trips~Mean_Humidity+MeanDew_Point_F+Precipitation_In+Mean_Wind_Speed_MPH+fall_winter+weekday_weekend,data = train_data)

predictions=predict(veryfinalmodeltest,newdata=test_data)

mae = mean(abs(predictions-test_data$total_trips))

mse = sqrt(mean((predictions-test_data$total_trips)^2))

mae

mse

predictionsst=sum((test_data$total_trips-mean(test_data$total_trips))^2)

predictionsse = sum((test_data$total_trips-predictions)^2)

rsq = 1 - (predictionsse/predictionsst)

```


Each regressor is significant to at least the 0.01 level, and the diagnostic plots are satisfactory. The residual plot moving-average looks flat. The normal quantile plot has few departures from the line. The scale location plot is relatively flat, indicating constant variance across fitted values. 







```{r, include=FALSE}
cor(df$total_durations,df$total_trips)
#Total trips and total durations are only 82.2% correlated

cor(df$total_durations, df$Mean_Temperature_F, use = "complete.obs")

cor(df$total_durations, df$MeanDew_Point_F)


```


# Conclusion
## Interpretations
For a given day with other variables held fixed, our model dictates
\begin{itemize}
\item For a 1\% increase in Mean Relative Humidity we expect 6.2 fewer trips
\item For a 1°F increase in Mean Dew Point, we expect 8.1 more trips
\item For a 1 inch increase in Precipitation we expect 103.5 fewer trips
\item For a 1 MPH increase in the Mean Wind Speed, we expect 8.8 fewer trips
\item For a day falling in between (including) 22 September and 19 March, we expect 34.1 fewer trips than outside of those days
\item For a weekend day, we expect 50.2 fewer trips than on a weekday
\end{itemize}

It makes sense for trips to drop as relative humidity rises, because higher relative humidity is uncomfortable to riders. It makes sense for trips to increase as the Mean Dew Point increases, as a higher Dew Point generally signals warmer weather. It makes sense that any precipitation would flatten ridership, as biking in rain or snow could be more risky and uncomfortable. It makes sense that an increase in the miles per hour of mean wind speed would decrease ridership, as faster winds increase wind chill which feels very uncomfortable, and biking into headwinds is tiresome. It makes sense that ridership decreases in the fall and winter, as this is when days are shorter, when Seattle's rainy season starts, and when the leaves fall, potentially making it so that biking is slippery and less scenic. Lastly, an explanation for why ridership drops on the weekend is that many people likely used Pronto to get to work, which would not happen on the weekends.

## Limitations
In terms of limitations, as stated before, given that the climate of Seattle is different from many other cities, our results will have limited generalizability. Another limitation is that the weather data is for all of the Puget sound region, while the biking data is concentrated in downtown Seattle. If the weather data was specialized to the city itself, we may see the weather data have higher explanatory power. Lastly, one limitation is that two of are variables, mean humidity and mean dew point, are mathematically related, which could be cause for concern. The reason that this relationship doesn't affect the regression model is that they are not \textit{linearly} related.

## Extensions
Some possible extensions of our work here could be analyzing the relationships between weather and ridership in different cities. We might see different relationships between the weather and ridership in cities that have different climates than Seattle, for instance, Houston, Texas. Another potential extension would be to use interaction terms, and create a dummy variable for holidays, like a 2018 paper by Kyoungok Kim did. 

# References 

Pronto Cycle Share, 2016. "Pronto Cycle Share Dataset," Kaggle. Available online at: https://www.kaggle.com/datasets/pronto/cycle-share-dataset.

Kim, Kyoungok, 2018. "Investigation on the effects of weather and calendar events on bike-sharing according to the trip patterns of bike rentals of stations," Journal of Transport Geography, Elsevier, vol. 66(C), pages 309-320. 

An, Ran & Zahnow, Renee & Pojani, Dorina & Corcoran, Jonathan, 2019. "Weather and cycling in New York: The case of Citibike," Journal of Transport Geography, Elsevier, vol. 77(C), pages 97-112.

University of Washington, 2019. "Bike-sharing in Seattle," UW News, October 7, 2019. Available online.

# Appendix
## Code Used for Data Cleaning
```{r mapping 1 copy, eval=FALSE}
trip = read_csv('pronto-cycle-share-trip-data.csv')
# map unique dates to integers starting at 1
# strips the date from its current format
trip$date <- as.Date(trip$starttime, format = "%m/%d/%Y %H:%M") 
unique_dates <- sort(unique(trip$date)) # this collects unique dates
# this maps unique date to the integers, starting at 1
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates)) 
# this adds the integer mapping as a column, day_number
trip$day_number = date_to_number[as.character(trip$date)] 
trip$count = 1 # this adds a one to each obs; useful for add
trip = dplyr::select(trip, count, tripduration, day_number)
```

```{r mapping 2 copy, eval=FALSE}
# construct new df, ridership, that aggregates trips by day
ridership = trip %>% group_by(day_number) %>%
  summarise(total_trips = sum(count),
            total_durations = round(sum(tripduration), 1),
            .groups = 'drop'); dim(ridership)
```

```{r clean_data copy, eval = FALSE}
weather = read_csv('weather.csv.xls')
# calculates temperature range for each day
weather$temp_range = weather$Max_Temperature_F - weather$Min_TemperatureF 
# strips the date from its current format
weather$date <- as.Date(weather$Date, format = "%m/%d/%Y") 
# maps unique date to the integers, like the chunk above
date_to_number <- setNames(seq_along(unique_dates), as.character(unique_dates))
weather$day_number = date_to_number[as.character(weather$date)]
weather = weather[,-1] # remove the old date
# this will be our data frame going forward
df = left_join(weather,ridership, by='day_number'); dim(df)
df$avg_durations = round(df$total_durations / df$total_trips, 1)
```

```{r add season variables copy, eval = FALSE}
df$weekday_weekend <- ifelse(weekdays(df$date) %in% c("Saturday", "Sunday"),1,0)
# Define a function to classify seasons based on actual start dates
get_season <- function(date) {
  year <- lubridate::year(date)
  spring_start <- as.Date(paste0(year, "-03-20"))
  summer_start <- as.Date(paste0(year, "-06-21"))
  fall_start   <- as.Date(paste0(year, "-09-22"))
  winter_start <- as.Date(paste0(year, "-12-21"))
  ifelse(date >= spring_start & date < summer_start, 0,  # Spring
  ifelse(date >= summer_start & date < fall_start,   1,  # Summer
  ifelse(date >= fall_start & date < winter_start,   2,  # Fall
  3)))  # Winter
}
# Apply the function to the 'date' variable
df$season <- sapply(df$date, get_season)
# Create fall/winter dummy: 1 if Fall or Winter, 0 otherwise
df$fall_winter <- ifelse(df$season %in% c(2, 3), 1, 0)
```
## Summary Statistics for Final Model
```{r summary very final model, echo=FALSE}
summary(veryfinalmodel)
```

## Diagnostic Plots for Final Model
```{r plot very final model, echo=FALSE}
par(mfrow= c(2,2))
plot(veryfinalmodel)
```
